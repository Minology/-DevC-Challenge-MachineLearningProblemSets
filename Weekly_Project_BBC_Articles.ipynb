{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Weekly-Project-BBC Articles.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8btHNo7H5Cf"
      },
      "source": [
        "# Organize ML projects with Scikit-Learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yK9CuWlH5Ch"
      },
      "source": [
        "While Machine Learning is powerful, people often overestimate it: apply machine learning to your project, and all your problems will be solved. In reality, it's not this simple. To be effective, one needs to organize the work very well. In this notebook, we will walkthrough practical aspects of a ML project. To look at the big picture, let's start with a checklist below. It should work reasonably well for most ML projects, but make sure to adapt it to your needs:\n",
        "\n",
        "1. **Define the scope of work and objective**\n",
        "    * How is your solution be used?\n",
        "    * How should performance be measured? Are there any contraints?\n",
        "    * How would the problem be solved manually?\n",
        "    * List the available assumptions, and verify if possible.\n",
        "    \n",
        "    \n",
        "2. **Get the data**\n",
        "    * Document where you can get that data\n",
        "    * Store data in a workspace you can easily access\n",
        "    * Convert the data to a format you can easily manipulate\n",
        "    * Check the overview (size, type, sample, description, statistics)\n",
        "    * Data cleaning\n",
        "    \n",
        "    \n",
        "3. **EDA & Data transformation**\n",
        "    * Study each attribute and its characteristics (missing values, type of distribution, usefulness)\n",
        "    * Visualize the data\n",
        "    * Study the correlations between attributes\n",
        "    * Feature selection, Feature Engineering, Feature scaling\n",
        "    * Write functions for all data transformations\n",
        "    \n",
        "    \n",
        "4. **Train models**\n",
        "    * Automate as much as possible\n",
        "    * Train promising models quickly using standard parameters. Measure and compare their performance\n",
        "    * Analyze the errors the models make\n",
        "    * Shortlist the top three of five most promising models, preferring models that make different types of errors.\n",
        "\n",
        "\n",
        "5. **Fine-tunning**\n",
        "    * Treat data transformation choices as hyperparameters, expecially when you are not sure about them (e.g., replace missing values with zeros or with the median value)\n",
        "    * Unless there are very few hyperparameter value to explore, prefer random search over grid search.\n",
        "    * Try ensemble methods\n",
        "    * Test your final model on the test set to estimate the generalizaiton error. Don't tweak your model again, you would start overfitting the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofeuKevOH5Ch"
      },
      "source": [
        "## Example: Articles categorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2NSUqUEH5Ci"
      },
      "source": [
        "### Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GttlMG-H5Cj"
      },
      "source": [
        "Build a model to determine the categories of articles. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwbjWOG1H5Ck"
      },
      "source": [
        "### Get Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWq7xex_H5Ck"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sns.set_style(\"whitegrid\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9W7Hzt2H5Cp"
      },
      "source": [
        "data = pd.read_csv('https://raw.githubusercontent.com/dhminh1024/practice_datasets/master/bbc-text.csv')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teb1QvD1H5Cs",
        "outputId": "0f0992a5-56e3-4831-eaa7-8d2a79c84898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "data.sample(5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1234</th>\n",
              "      <td>business</td>\n",
              "      <td>verizon  seals takeover of mci  verizon has wo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1563</th>\n",
              "      <td>business</td>\n",
              "      <td>chinese wine tempts italy s illva italy s illv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>632</th>\n",
              "      <td>tech</td>\n",
              "      <td>us woman sues over ink cartridges a us woman i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2001</th>\n",
              "      <td>business</td>\n",
              "      <td>china continues rapid growth china s economy h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1331</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>glastonbury fans to get id cards fans who buy ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           category                                               text\n",
              "1234       business  verizon  seals takeover of mci  verizon has wo...\n",
              "1563       business  chinese wine tempts italy s illva italy s illv...\n",
              "632            tech  us woman sues over ink cartridges a us woman i...\n",
              "2001       business  china continues rapid growth china s economy h...\n",
              "1331  entertainment  glastonbury fans to get id cards fans who buy ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBW_Sg2RH5Cy",
        "outputId": "57f0193c-f41a-4a5a-fd9a-13b6295ae68b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2225 entries, 0 to 2224\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   category  2225 non-null   object\n",
            " 1   text      2225 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 34.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh3VRY5Zxmw6",
        "outputId": "a2a7517b-88f1-4791-b812-89a787425d2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-V_M0Iqa9sV"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76Al9rs8a_zt"
      },
      "source": [
        "import re\n",
        "def preprocessor(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
        "    text = re.sub('[0-9.?!]', '', text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l2BNEeabCmB"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def tokenizer(text):\n",
        "    return text.split()\n",
        "\n",
        "def tokenizer_porter(text):\n",
        "    return [porter.stem(word) for word in text.split()]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRx3t42cbEUN"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data['text']\n",
        "y = data['category']"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8MvSaVybMaJ",
        "outputId": "76af1001-3466-42dc-e6f7-0c54e982c8f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words=stop_words,\n",
        "                        tokenizer=tokenizer_porter,\n",
        "                        preprocessor=preprocessor)\n",
        "\n",
        "X = tfidf.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=102)\n",
        "clf = LogisticRegression(random_state=0)\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx7bFRI_bV59",
        "outputId": "1d638ed1-c591-40d2-e6f0-0abe97f56d3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "predictions = clf.predict(X_test)\n",
        "print('accuracy:', accuracy_score(y_test, predictions))\n",
        "print('confusion matrix:\\n', confusion_matrix(y_test, predictions))\n",
        "print('classification report:\\n', classification_report(y_test, predictions))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.9820224719101124\n",
            "confusion matrix:\n",
            " [[88  1  1  0  0]\n",
            " [ 0 91  0  0  1]\n",
            " [ 2  0 82  1  0]\n",
            " [ 1  0  0 98  0]\n",
            " [ 0  0  1  0 78]]\n",
            "classification report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     business       0.97      0.98      0.97        90\n",
            "entertainment       0.99      0.99      0.99        92\n",
            "     politics       0.98      0.96      0.97        85\n",
            "        sport       0.99      0.99      0.99        99\n",
            "         tech       0.99      0.99      0.99        79\n",
            "\n",
            "     accuracy                           0.98       445\n",
            "    macro avg       0.98      0.98      0.98       445\n",
            " weighted avg       0.98      0.98      0.98       445\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}